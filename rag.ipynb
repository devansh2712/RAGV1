{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document as DocxDocument\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.schema.document import Document\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your OpenAI API key (assumed to be stored in an environment variable)\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # Retrieve the key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client=OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vectors):\n",
    "    \"\"\"\n",
    "    Normalize a list of vectors.\n",
    "\n",
    "    Args:\n",
    "        vectors (list): A list of vectors to normalize.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of normalized vectors.\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_pdf_text_and_page_info(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF and returns a dictionary with page number and text.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are page numbers (starting from 1) and values are the text of that page.\n",
    "    \"\"\"\n",
    "    pdf_text_by_page = {}\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        for page_num in range(1, num_pages + 1):  # Page numbers start from 1\n",
    "            page_obj = pdf_reader.pages[page_num - 1]  # Adjust for zero-based indexing\n",
    "            page_text = page_obj.extract_text()\n",
    "            pdf_text_by_page[page_num] = page_text\n",
    "\n",
    "    return pdf_text_by_page\n",
    "\n",
    "\n",
    "# Function to extract text from Word files\n",
    "def extract_docx_text_and_page_info(docx_path):\n",
    "    doc = DocxDocument(docx_path)\n",
    "    text_by_page = {}\n",
    "\n",
    "    paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]  # Remove empty lines\n",
    "    full_text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    # Treat the document as a single \"page\"\n",
    "    text_by_page[1] = full_text\n",
    "    return text_by_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a FAISS index\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List your documents here (adjust paths as needed)\n",
    "\"\"\"document_paths = [os.path.join(\"data\", f) for f in os.listdir(\"data\") if f.endswith((\".pdf\"))]\"\"\"\n",
    "\n",
    "document_paths = [\n",
    "    os.path.join(\"data\", f) for f in os.listdir(\"data\") if f.endswith((\".pdf\", \".docx\"))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for pdf_file_path in document_paths:\n",
    "    # Extract text and page info from the PDF\n",
    "    pdf_data = extract_pdf_text_and_page_info(pdf_file_path)\n",
    "\n",
    "    for key, value in pdf_data.items():\n",
    "        docs = [Document(page_content=value, metadata={\"page_number\": key, \"source\": pdf_file_path})]\n",
    "        embedding = embeddings.embed_query(value)\n",
    "        normalized_embedding = normalize([embedding])[0]\n",
    "        vector_store.add_documents(documents=docs, embeddings=[normalized_embedding])\"\"\"\n",
    "\n",
    "\n",
    "for file_path in document_paths:\n",
    "    # Extract text based on file type\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        file_data = extract_pdf_text_and_page_info(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        file_data = extract_docx_text_and_page_info(file_path)\n",
    "\n",
    "    # Process extracted text and add to vector store\n",
    "    for page_num, text in file_data.items():\n",
    "        docs = [Document(page_content=text, metadata={\"page_number\": page_num, \"source\": file_path})]\n",
    "        embedding = embeddings.embed_query(text)\n",
    "        normalized_embedding = normalize([embedding])[0]\n",
    "        vector_store.add_documents(documents=docs, embeddings=[normalized_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_response(query):\n",
    "    embedding = embeddings.embed_query(query)\n",
    "    normalized_embedding = normalize([embedding])[0]\n",
    "    #result_with_score = vector_store.similarity_search_with_score(query, k=1, embeddings=[normalized_embedding])\n",
    "    result_with_score = vector_store.similarity_search_by_vector(normalized_embedding, k=1)\n",
    "    prompt =f\"\"\"\n",
    "    You are a helpful assistan. Your task is to understand the rag context along with the user query and then reply to the user based on the context.\n",
    "    The user query is: {query}\n",
    "    The context from doing rag based on the user query is: {result_with_score[0].page_content}\n",
    "    Your task is to reply to the user based on the context and the user query.\n",
    "    \"\"\"\n",
    "    output_response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    llm_response = output_response.choices[0].message.content\n",
    "    return llm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transcript of Mandar Badve's conversation primarily revolves around his experience and insights into using the autogen framework for building agent frameworks. Mandar has 14 years of experience as a software engineer and is currently focused on developing agent frameworks and tools using\n"
     ]
    }
   ],
   "source": [
    "#### RUN QUERY HERE ####\n",
    "query = \"Summarise Transcript of Mandar Bandve\"\n",
    "response = query_response(query)\n",
    "print(response)  # Print the response from the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devansh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
